---
title: "Permutation"
author: "Lesley Potters"
date: "11/30/2021"
slug: permutation
tags:
  - Permutation
  - Machine Learning
  - Feature engineering
  - Python
 
output: 
  blogdown::html_page:
    toc: true
    toc_depth: 4
    
---
*Permutation* (per + mutare, or ‘thoroughly change’) is a fantastic way to check the importance of the features in your model.

The idea behind it is very clever and simple at the same time: for each of the features you randomly shuffle the values per observation and you calculate what the impact is on the output. By adding this randomness in your input data, some hidden characteristics might be revealed.

Below you find a simple visualization to make things clearer. This is using the sklearn dataset on diabetes, a toy dataset with 10 features (age, sex, bmi, blood pressure and six blood serum measurements s1…s6) for 442 patients and as target a quantitative measure of disease progression one year after baseline.

For each of the features, there will be K permutations of randomly shuffling the values around, while keeping the rest the same.

![](/images/permutare.JPG){width=100%}

If the model score (measured by accuracy, precision, average loss, etc.) decreases, it seems that the true input has some good predictive power. If the shuffling does not have any effect on your model score (or it even improves your model’s score), the model works equally well or even better with the random (made up!) data than with the real data, so these feature are of little importance.

Important to note is that permutation is done at the end of fitting the model. This is not a method to check how your features perform before building your model. Some features that might be of limited importance to a certain model might be of high importance to a different model.

## The math
The math behind permutation is rather straightforward. After fitting a model with all features and the real data, the permutation feature importance for each feature can then be computed as follows:

Input: the fitted model and its data (for example an i × j matrix with i observations and j features)
Reference: the model score s
Computation:
For each feature j in the data we shuffle the data K times, which gives us j × k versions of the new data.
For each feature and each permutation, we compute the new model’s score sk,j using the corrupted data.
Then we can finally compute the importance i for feature j by substracting the average of the models scores sk,j (for each permutation k of feature j) from the original model score s: permimportanceij
A quick example
Let’s show an example, again using the sklearn diabetes dataset. First, we import the library and the data:

Prepare the data:
```{r test-python, engine='python'}
import eli5
import sklearn
import pandas as pd

from sklearn.datasets import load_diabetes
X, Y = load_diabetes(return_X_y=True,as_frame=True)
 
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)
```

Create a baseline model for sake of simplicity:

```{r test-python, engine='python'}
from sklearn.ensemble import RandomForestRegressor
baseline_model = RandomForestRegressor(n_estimators=100,random_state=0).fit(X_train, Y_train)
```

Perform the permutation and show the scores using the eli5 library:

```{r test-python, engine='python'}
from eli5.sklearn import PermutationImportance
perm = PermutationImportance(baseline_model).fit(X_test, Y_test)
eli5.show_weights(perm,feature_names=X_train.columns.to_list())
```

## Interpretation
The values at the top of the list are the most important features, in this case s5 (ltg, the log of serum triglycerides level) and bmi (body mass index) are the most important predictors on how diabetes will evolve. The first number in each row shows how much model performance decreased on average with the K permutations, followed by an interval over the permutations. On the other hand s3 (high-density lipoproteins) has almost no importance as a feature and could be deleted in this model specification.